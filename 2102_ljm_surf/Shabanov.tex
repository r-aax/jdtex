\documentclass[
11pt,%
tightenlines,%
twoside,%
onecolumn,%
nofloats,%
nobibnotes,%
nofootinbib,%
superscriptaddress,%
noshowpacs,%
centertags]%
{revtex4}
\usepackage{ljm}
\usepackage{listings}
\usepackage{amsmath}

\lstset{
language=C++,
basewidth=0.5em,
xleftmargin=45pt,
xrightmargin=45pt,
basicstyle=\small\ttfamily,
keywordstyle=\bfseries\underbar,
numbers=left,
numberstyle=\tiny,
stepnumber=1,
numbersep=10pt,
showspaces=false,
showstringspaces=false,
showtabs=false,
frame=trBL,
tabsize=2,
captionpos=t,
breaklines=true,
breakatwhitespace=false,
escapeinside={\%*}{*)}
}

\begin{document}

\titlerunning{Scaling of supercomputer calculations}
\authorrunning{Shabanov et al.}

\title{Исследование эффективности масштабирования суперкомпьютерных вычислений на неструктурированных поверхностных расчетных сетках}

\author{\firstname{B.~M.}~\surname{Shabanov}}
\email[E-mail: ]{shabanov@jscc.com}
\affiliation{Joint Supercomputer Center of the Russian Academy of Sciences -- branch of Scientific Research Institute of System Analysis of the Russian Academy of Sciences, Leninsky prospect 32a, Moscow, 119334, Russia}

\author{\firstname{A.~A.}~\surname{Rybakov}}
\email[E-mail: ]{rybakov.aax@gmail.com}
\affiliation{Joint Supercomputer Center of the Russian Academy of Sciences -- branch of Scientific Research Institute of System Analysis of the Russian Academy of Sciences, Leninsky prospect 32a, Moscow, 119334, Russia}

\author{\firstname{S.~S.}~\surname{Shumilin}}
\email[E-mail: ]{shumilin@jscc.com}
\affiliation{Joint Supercomputer Center of the Russian Academy of Sciences -- branch of Scientific Research Institute of System Analysis of the Russian Academy of Sciences, Leninsky prospect 32a, Moscow, 119334, Russia}

\author{\firstname{M.~Yu.}~\surname{Vorobyov}}
\email[E-mail: ]{nordmike@jscc.com}
\affiliation{Joint Supercomputer Center of the Russian Academy of Sciences -- branch of Scientific Research Institute of System Analysis of the Russian Academy of Sciences, Leninsky prospect 32a, Moscow, 119334, Russia}

\firstcollaboration{(Submitted by A.~M.~Elizarov)} % Add if you know submitter.
%\lastcollaboration{ }

\received{TODO}

\begin{abstract}
При решении комплексных задач численного моделирования используются расчетные сетки, содержащие десятки и сотни миллионов ячеек.
Современные задачи и вовсе переходят черту в миллиард ячеек.
Локальные вычислительные станции не способны справиться с такими объемами данных и вычислений.
Для выполнения вычислений такого объема требуется использование суперкомпьютерных кластеров, состоящих из многих вычислительных узлов, связанных между собой высокоскоростной коммуникационной сетью.
При этом необходимо выполнять декомпозицию расчетной сетки на отдельные домены, чтобы обеспечить ее параллельную обработку сразу на всех узлах кластера.
Эти домены распределяются по вычислительным узлам суперкомпьютера и обрабатываются независимо друг от друга.
Для синхронизации вычислений после каждой итерации обработки ячеек производятся обмены данными на границах между соседними соприкасающимися доменами.
Для эффективности выполнения вычислений и масштабирования их на большое количество вычислительных узлов требуется разработка эффективных алгоритмов декомпозиции расчетных сеток, порождающих множество доменов с накладываемыми на них требованиями по количеству ячеек, равномерности распределения ячеек по доменам, связности доменов и размеру границ между ними.
При этом теоретические показатели эффективности декомпозиции расчетной сетки не гарантируют эффективного выполнения реальной задачи на суперкомпьютере.
В данной статье в качестве объекта исследования рассматривается неструктурированная поверхностная сетка, используемая для расчета процессов взаимодействия объемного тела с окружающей средой.
Для сетки такого вида рассматривается иерархический алгоритм декомпозиции с выбором оптимального критерия разбиения на домены.
С использованием данного алгоритма декомпозиции проводятся суперкомпьютерные расчеты на вычислительных ресурсах МСЦ РАН с целью измерения практических показателей масштабируемости высоконагруженных приложений.
\end{abstract}

\subclass{65Y05,65Y20,49M27} % Enter 2010 Mathematics Subject Classification.

\keywords{суперкомпьютер, поверхностная неструктурированная расчетная сетка, декомпозиция, домен, высокопроизводительные вычисления, масштабирование вычислений}

\maketitle

\section{Introduction}

Современные расчетные приложения крайне требовательны к вычислительным ресурсам.
Для больших задач не представляется возможным выполнение их на отдельно взятом вычислителе (одном микропроцессоре или одном сервере) за приемлемое время.
Возникает потребность использовать для вычислений суперкомпьютерные кластеры, состоящие из многих вычислительных узлов.
Для того, чтобы выполнить задачу на суперкомпьютере, необходимо разделить ее расчетную область на отдельные подобласти, называемые доменами, и обрабатывать эти домены параллельно и независимо друг от друга.
Для повышения эффективности суперкомпьютерных приложений внутри вычислительного узла применяются различные методы подготовки данных и распараллеливания исполнения для систем с общей памятью, а также низкоуровневые оптимизации программного кода, такие как векторизация, позволяющие существенно повысить скорость выполнения приложений.
Конечно, на границах соприкосновения доменов возникает необходимость синхронизации вычислений, что достигается путем обмена данными (например, с использованием MPI).
Таким образом, выполнение суперкомпьютерных расчетов состоит из двух чередующихся шагов: параллельная обработка ячеек расчетной области и обмен данными на границах соприкосновения доменов, обрабатываемых различными процессами.
Эффективность выполнения суперкомпьютерных приложений существенным образом зависит от качества декомпозиции расчетной сетки и ее распределения между разными вычислительными узлами.

Данная статья посвящена задаче декомпозиции поверхностной неструктурированной расчетной сетки для распределения между узлами гомогенного суперкомпьютерного кластера (то есть кластера, состоящего из одинаковых вычислительных узлов) для повышения эффективности масштабирования вычислений.
Пусть дана поверхностная сетка, состоящая из $S$ расчетных ячеек, пусть также суперкомпьютер состоит из $n$ вычислительных узлов с одинаковыми характеристиками.
Также будем считать, что скорость обмена данными между любыми двумя вычислительными узлами одинакова для всех узлов.
Распространение задачи распределения вычислительной нагрузки на гетерогенный вычислительный кластер достигается путем ввода весовых коэффициентов для вычислительных узлов и для каналов обмена данными, как это описано в[TODO].
Если представить скорость обработки расчетных ячеек на одном вычислительном узле как $a^{-1}$, то время выполнения одной итерации расчетов на одном вычислительном узле будет равно $T_1 = aS$.
Пусть теперь расчетная область разбита на $n$ доменов, содержащих по $S_i$ ячеек ($i=1,n$).
Обозначим через $L_{ij}$ количество ребер, составляющих границу между доменами $S_i$ и $S_j$.
Будем считать, что каждый домен обрабатывается на своем вычислительном узле.
Таким образом, все домены обрабатываются параллельно, и время обработки всех ячеек определяется временем обработки самого крупного домена.
Кроме обработки всех расчетных ячеек после проведения итерации расчета необходимо выполнить обмен данными между всеми парами доменов по всем границам между ними.
Пусть скорость передачи данных между узлами определяется величиной $b^{-1}$, и все обмены выполняются параллельно, тогда время выполнения всех обменов определяется временем обмена данными через самую длинную границу.
Исходя из этого, можно определить суммарное время выполнения одной итерации расчета при выполнении на $n$ вычислительных узлах:

\begin{equation}
T_n = a \max_{i = 1,n}{S_i} + b \max_{i,j=1,n}{L_{ij}}
\end{equation}

Критерием оптимизации декомпозиции расчетной сетки является сокращение времени выполнения расчетов, то есть величины $T_n$.
Время выполнения расчетов напрямую зависит от размера самого крупного домена, однако мы будем рассматривать не абсолютный размер домена, а его отклонение от теоретического оптимального значения.
Очевидно, что в идеальном случае при декомпозиции все домены должны иметь размер $\frac{S}{n}$, а относительное отклонение от идеального размера в процентах можно вычислить по формуле

\begin{equation}
D = 100 \% \left( \frac{n}{S} \max_{i=1,n}{S_i} - 1 \right)
\end{equation}

Вторым важным критерием качества выполненной декомпозиции является наибольшее значение длины границы между парами доменов.
В данном случае можно использовать абсолютную характеристику, так как длину границы предсказать довольно сложно, и у нее вообще говоря нет теоретического минимума (в зависимости от геометрии рассматриваемой сетки теоретически длина границы между доменами может быть нулевой).
То есть в качестве критерия сравнения различных методов декомпозиции сетки будем использовать следующую величину:

\begin{equation}
L = \max_{i,j=1,n}{L_{ij}}
\end{equation}

Несмотря на то, что в наших предположениях все обмены данными между доменами выполняются одновременно, общий объем всех обменов существенно влияет на скорость обмена данными, поэтому этот параметр также необходимо учитывать.
Введем его в следующем виде.
Общее число ребер расчетной сетки остается неизменным вне зависимости от алгоритма декомпозиции и количества доменов (обозначим общее количество ребер сетки через $E$).
Среди этих ребер есть граничные ребра, имеющие только одну инцидентную ячейку, их количество также неизменно и равно $E_B$ (от слова "border").
Остальные ребра имеют две инцидентные ячейки.
Если обе ячейки, инцидентные некоторому ребру, принадлежат одному и тому же домену, то будем называть такое ребро внутренним ребром этого домена (обозначим их количество через $E_{INN}$ от слова "inner"), в противном случае ребро входит в границу между двумя доменами будем называть такое ребро междоменным (обозначим их количество через $E_{INT}$ от слова "inter").
Ребер других видов быть не может.
Таким образом, выполняется соотношение $E = E_B + E_{INN} + E_{INT}$.
В качестве параметра оценки качества декомпозиции будем рассматривать долю междоменных ребер в общем количестве ребер сетки, то есть величину

\begin{equation}
I = 100 \% \left( \frac{E_{INT}}{E} \right)
\end{equation}
	  
Для оценки качества декомпозиции расчетной сетки следует учитывать все три описанных параметра: $D$ -- отклонение размера максимального домена от идеального значения, $I$ -- долю междоменных ребер в общем числе ребер расчетной сетки и $L$ -- длину наиболее протяженной границы между парами доменов.
Чем ниже значения данных критериев, тем более качественной является декомпозиция и тем более эффективного выполнения расчетов стоит ожидать при выполнении запусков на реальной машине.

\section{Распараллеливание вычислений на неструктурированной расчетной сетке}

\begin{figure}[h]
\includegraphics[width=1.0\textwidth]{pics/02-scheme.pdf}
\captionstyle{center}\caption{Схема распараллеливания вычислений на суперкомпьютере.}\label{fig:02-scheme}
\end{figure}

Рассмотрим схему распараллеливания вычислений, связанных с моделированием взаимодействия объемного тела с окружающей средой.
При этом вычисления проводятся на поверхности взаимодействующего тела, описываемой неструктурированной поверхностной расчетной сеткой (GSU - Grid Surface Unstructured) с треугольными ячейками.
В рассматриваемой схеме вычислений на поверхности тела рассматриваются различные физические процессы, включая деформацию поверхности, течение жидкой пленки и образование ледяного нароста, что в конечном итоге должно приводить к перестроению расчетной поверхности.
Схема вычислений, таким образом, разделяется на несколько независимых функциональных модулей (схема взаимодействия модулей показана на Fig~\ref{fig:02-scheme}).

Первый модуль -- \texttt{gsu-split} -- предназначен для загрузки расчетной сетки, ее декомпозиции на несколько доменов и подготовке ее к передаче решателю.
Физический решатель \texttt{solver} предназначен только для проведения высоконагруженных вычислений, он взаимодействует с суперкомпьютерным кластером и работает на произвольном количестве вычислительных узлов.
Это наиболее требовательная к ресурсам часть приложения, занимающая более 90\% расчетного времени, поэтому она реализуется с использованием механизмов MPI, OpenMP и векторизации программного кода.
После проведения необходимых расчетов модуль \texttt{solver} возвращает накопленные расчетные данные с разных временных точек, которые поступают на вход модулю слияния данных \texttt{gsu-merge}.
Последним звеном в цепочке выполнения расчетов является модуль \texttt{gsu-remesh}, который на вход получает расчетную сетку (в общем случае множество расчетных сеток с разных точек времени) и выполняет ее перестроение. 

\section{Декомпозиция неструктурированной расчетной сетки}

В работе [TODO] описан параллельный алгоритм геометрической декомпозиции сеточных данных.
Во время работы данного алгоритма происходит последовательное деление текущего домена пополам с помощью сечения плоскостью.
По логике данного алгоритма изначальный головной домен (\texttt{h} -- head) делится на пару доменов \texttt{hl} (left), \texttt{hr} (right), каждый из которых делится далее пополам и так далее на любое количество доменов, равное степени двойки.

Данный алгоритм предлагается расширить, введя в него произвольные критерии разбиения текущего домена на пару более мелких доменов.
Вначале рассмотрим схему простого деления домена пополам с использованием произвольного признака, по которому производится деление (Fig.~\ref{fig:03-split}).

\begin{figure}[h]
\includegraphics[width=1.0\textwidth]{pics/03-split.pdf}
\captionstyle{center}\caption{Схема разделения домена пополам по выбранному признаку fun.}\label{fig:03-split}
\end{figure}

Пусть задано множество ячеек домена \texttt{h} и произвольная функция извлечения признака из ячейки \texttt{fun}.
Первым шагом является вычисление множества признаков для всех ячеек (\texttt{s} -- signs):
\begin{equation*}
	s = \{fun(h_i)~|~h_i \in h\}.
\end{equation*}

Сортируем \texttt{s}.

В отсортированном множестве признаков следует выбрать медианное значение (\texttt{b} -- blade):
\begin{equation*}
	b = median(s).
\end{equation*}

Данное значение будет использоваться для разделения домена на два меньших домена (\texttt{hl} -- head left, \texttt{hr} -- head right) с помощью применения двух простых фильтров:
\begin{equation*}
	hl = \{h_i~:~fun(h_i) < b,~h_i \in h\},
\end{equation*}
\begin{equation*}
	hr = \{h_i~:~fun(h_i) \geq b,~h_i \in h\}.
\end{equation*}

После разделения домена на два более мелких домена можно вычислить параметр, отражающий эффективность разбиения.
В качестве такого параметра предлагается использовать длину границы между двумя образованными новыми доменами.
Таким образом, критерий разбиения зависит от функции вычисления признака \texttt{fun}.
В свою очередь это означает, что при выполнении разбиения не обязательно ограничиваться одной функцией вычисления признака. Вместо этого можно подать список функций, для каждой функции вычислить показатель качества разбиения и в результате остановиться на той функции вычисления признака, которая в конечном итоге приводит к наиболее эффективному разбиению.
Если в качестве функций вычисления признака ячейки использовать просто извлечение трех координат центров ячеек, то мы получим в чистом виде алгоритм геометрической декомпозиции сетки с выбором для дробления наиболее протяженного размера по одной из координат.
Результаты применения данного алгоритма можно видеть на Fig.~\ref{fig:03-explode-bunny} и Fig.~\ref{fig:03-hierarch}.

\begin{figure}[h]
\includegraphics[width=1.0\textwidth]{pics/03-explode-bunny.pdf}
\captionstyle{center}\caption{Пример декомпозиции поверхностной расчетной сетки с помощью иерархического алгоритма.}\label{fig:03-explode-bunny}
\end{figure}

\begin{figure}[h]
\includegraphics[width=1.0\textwidth]{pics/03-hierarch.pdf}
\captionstyle{center}\caption{Примеры декомпозиции поверхностной расчетной сетки с помощью иерархического алгоритма.}\label{fig:03-hierarch}
\end{figure}

Варьируя набор функций вычисления признаков, по которым можно выполнять разбиение домена, возможно выполнять геометрическую декомпозицию вдоль направления любой кривой, для которой вычисляется проекция ячейки.
Декомпозиция с помощью данного метода не ограничивается только геометрическими признаками.
Функции вычисления признаков могут использоваться для анализа физических данных ячеек, например, для локализации и выделения в отдельные домены областей с повышенным давлением.
В рамках данной работы описанный алгоритм декомпозиции поверхностной неструктурированной расчетной сетки применялся к поверхностным сеткам, используемым для расчета обледенения поверхности летательного аппарата.
При расчете обледенения летательного аппарата основной объем вычислений относится к обработке поверхностных ячеек, общие данные между соседними доменами собраны на междоменных ребрах, что обеспечивает небольшой обмен данными, которыми требуется обмениваться в ходе синхронизации вычислений.
Характерный размер таких сеток составил около $10^5$ ячеек, рассматривались как односвязные, так и многосвязные поверхности, а также поверхности, состоящие из нескольких изолированных друг от друга зон.

Стоит отметить, что теоретические параметры качества $D$, $L$, $I$ получаются достаточно низкие.
Параметр $D$ практически равен нулю, так как на каждом шаге деление домена выполняется строго пополам.
Отклонения же в длинах границ между доменами и суммарная протяженность границ также получается приемлемой несмотря на то, что алгоритм не гарантирует образование доменов с минимальными границами или даже связных доменов (в наихудших случаях возможно возникновение доменов произвольной формы и состоящих из произвольного количества изолированных частей поверхности).
Конечно, существенным ограничением алгоритма является то, что с помощью него возможно разбиение поверхности только на количество частей, являющееся степенью двойки, однако близкое к нулю значение показателя качества балансировки вычислительной нагрузки $D$ позволяет в данном случае мириться с этим недостатком.

\section{Организация межпроцессных обменов}

\begin{figure}[h]
\includegraphics[width=1.0\textwidth]{pics/04-MPI.pdf}
\captionstyle{center}\caption{Схема выполнения MPI-обменов через границу двух зон.}\label{fig:04-MPI}
\end{figure}

При проведении расчетов на декомпозированной сетке на каждой итерации счета необходимо выполнять обмен данными на каждой границе между доменами.
В нашем случае при выполнении декомпозиции расчетной сетки граница между двумя доменами представлена произвольным набором междоменных ребер.
При этом граница может быть разрывной, она и вовсе может состоять из отдельных ребер, поэтому последовательность междоменных ребер при описании границы не имеет значения.

На Fig.~\ref{fig:04-MPI} представлена схема организации межпроцессных обменов.
На данной иллюстрации два домена (будем условно их называть левым и правым), обрабатывающиеся в MPI-процессах с номерами 0 и 1, разделены границей, состоящей из трех ребер.
При этом в левом домене присутствует три ячейки, которые примыкают к рассматриваемой границе, в правом домене таких ячеек только две (так как ячейка 23R примыкает сразу к двум ребрам границы).
Для организации межпроцессных обменов в каждом домене для каждого ребра рассматриваемой границы создаются фиктивные ячейки, которые участвуют при расчете потоков через ребра границы.
При этом пересчет физических величин в фиктивных ячейках выполнять не требуется, все данные для фиктивных ячеек получаются с помощью MPI-обменов из настоящих ячеек соседнего домена.

Для пересылки данных из настоящих граничных ячеек домена в фиктивные ячейки соседнего домена в каждом из двух соседних доменов организуются буферы отправки и приема данных.
Последовательность обмена данными выглядит следующим образом (схема показана на Fig.~\ref{fig:04-MPI}).
Сначала данные из настоящих граничных ячеек записываются в соответствующие буферы отправки данных (send buff), далее для всех границ расчетной сетки выполняются асинхронные команды \texttt{MPI\_Irecv} приема сообщений в буферах получения данных (recv buff).
После чего также одновременно для всех границ расчетной сетки выполняются команды асинхронной отправки данных \texttt{MPI\_Isend} из буферов отправки.
Далее выполняется ожидание завершения всех асинхронных обменов данными с помощью функции \texttt{MPI\_Waitall}.
Последним шагом, завершающим обмен данными между соседними доменами, является перенос полученных физических значений из буферов получения данных (recv buff) в соответствующие фиктивные ячейки.

Можно отметить, что при использовании фиктивных ячеек возможно некоторое дублирование данных.
Например, на представленной схеме ячейке 23R из правого домена соответствуют сразу две фиктивные ячейки в левом домене.
Эти ячейки содержат одинаковые данные.
Данное дублирование информации является приемлемым, так как данные фиктивных ячеек используются только на чтение для выполнения вычисления потоков через границу доменов, поэтому в данном случае выполнять какую-либо синхронизацию одинаковых фиктивных ячеек не нужно.

\section{Эффективность масштабирования вычислений на суперкомпьютере}

Для замеров показателей масштабируемости вычислений на неструктурированной поверхностной расчетной сетке была использована тестовая поверхность обтекаемого трехмерного тела, содержащая порядка $2 \cdot 10^5$ узлов и $4 \cdot 10^5$ ячеек.
В ячейках выполнялись расчеты, связанные с моделирование течения жидкой пленки, решением уравнений теплового баланса на поверхности, а также перестроение и сглаживание поверхности.
Для декомпозиции поверхностной сетки использовался простой иерархических алгоритм деления доменов пополам, описанный в данной статье, в котором в качестве признаков ячеек брались три координаты центра.
При этом в результате критерием выбора конкретной координаты для деления домена являлась минимизация длины границы между двумя доменами (такой подход позволяет выполнять деление домена по наиболее протяженному направлению).

\begin{table}[!h]
\label{tbl:supercomputers}
\setcaptionmargin{0mm}
\onelinecaptionsfalse
\captionstyle{flushleft}
\caption{Конфигурации сегментов суперкомпьютера МВС-10П ОП, на которых производились замеры масштабирования вычислений.}
\bigskip
\begin{tabular}{|c|c|c|c|c|}
\hline
\parbox{3.5cm}{\textit{Семейство\\микропроцессоров Intel}} & \parbox{4.0cm}{\textit{Количество\\процессоров / ядер /\\потоков в узле}} & \parbox{3.0cm}{\textit{Частота\\микропроцессора}} & \parbox{3.0cm}{\textit{Объем\\оперативной\\памяти в узле}} & \parbox{2.0cm}{\textit{Поддержка\\AVX-512}} \\
\hline
Xeon Broadwell & 2 / 32 / 64 & 2.6 GHz & 128 GB & no \\
\hline
Xeon Phi KNL & 1 / 72 / 288 & 1.5 GHz & 96 GB & yes \\
\hline
Xeon Skylake & 2 / 36 / 72 & 3.0 GHz & 192 GB & yes \\
\hline
Xeon Cascade Lake & 2 / 48 / 96 & 3.0 GHz & 192 GB & yes \\
\hline
\end{tabular}
\label{tab:supercomputers}
\end{table}   

Для измерения показателей масштабируемости вычислений использовались гомогенные сегменты вычислительной системы Межведомственного суперкомпьютерного центра РАН.
Всего расчеты проводились на четырех вычислительных сегментах, характеристики узлов которых приведены в таблице \ref{tab:supercomputers}.
В данной таблице можно отметить, что все микропроцессоры кроме Xeon Broadwell поддерживают набор инструкций AVX-512, позволяющий использовать специальные 512-битные векторные регистры для эффективной векторизации кода.
Также следует выделить вычислительные узлы на базе микропроцессора Xeon Phi KNL.
Эти микропроцессоры отличаются огромным количеством вычислительных ядер, каждое из которых способно выполнять до 4 потоков, что позволяет эффективно распараллеливать расчетные приложения вплоть до 288 потоков на одном микропроцессоре.

\begin{figure}[h]
\includegraphics[width=0.8\textwidth]{pics/speedup.pdf}
\captionstyle{center}\caption{Ускорение вычислений на суперкомпьютерах МСЦ РАН при увеличении количества узлов.}\label{fig:speedup}
\end{figure}

Основной целью выполняемых запусков было проведение замеров показателей сильной масштабируемости вычислений при полном распараллеливании внутри вычислительных узлов с использованием OpenMP.
То есть для всех запусков использовалась одна и та же поверхность (которая дробилась на нужное количество вычислительных узлов), а также в расчетах были задействованы все потоки, доступные внутри вычислительных узлов. 

При проведении расчетов замеры выполнялись независимо для каждой вычислительной системы в отдельности.
Приведем описание измеряемых величин в процессе расчета для одной конкретной вычислительной системы.
В качестве эталонного времени использовалось время выполнения задачи на одном вычислительном узле: $t(1)$.
Также были выполнены замеры времени выполнения задач для количества вычислительных узлов, равного степени двойки (2, 4, 8, 16, 32, 64).
При этом ускорением на количестве узлов, равном $i$, считалось значение величины $s(i) = \frac{t(1)}{t(i)}$.
На Fig.~\ref{fig:speedup} приведены диаграммы ускорения вычислений при увеличении количества вычислительных узлов для разных вычислительных систем.

Кроме вычисления непосредственно ускорения выполнения кода производились расчеты эффективности масштабирования.
Под эффективности масштабирования вычислений в данном случае понимается величина $e(i) = \frac{s(i)}{i}$.
Физический смысл данного показателя заключается в следующем.
Можно считать, что в случае идеального распараллеливания вычислений при увеличении количества вычислительных узлов в $n$ раз время выполнения уменьшается ровно в $n$ раз.
Таким образом в случае идеального распараллеливания $s(i) = i$, а $e(i) = 1$.
Эффективность масштабирования вычислений является удобным индикатором качества создания исполняемого параллельного кода и сравнения между собой различных вычислительных систем.
Заметим, что вполне возможно проявление сверхлинейной масштабируемости (когда значение $e(i)$ поднимается выше единицы), однако это скорее исключение, чем ожидаемый эффект.

\begin{figure}[h]
\includegraphics[width=0.8\textwidth]{pics/scaling.pdf}
\captionstyle{center}\caption{Эффективность масштабирования вычислений на суперкомпьютерах МСЦ РАН при увеличении количества узлов.}\label{fig:speedup}
\end{figure}

На Fig.~\ref{scaling} представлена диаграмма эффективности масштабирования вычислений для различных вычислительных сегментов в зависимости от количества использованных вычислительных узлов.
Можно видеть, что для всех вычислительных систем эффективность масштабирования варьируется в районе значений 0,8-0,9, хотя на некоторых конфигурациях запуска наблюдаются провалы даже в район 0,7.
Запуски с низким значением эффективности распараллеливания как правило связаны с разбросом времени обработки MPI процессами своих доменов.
Несмотря на то, что использованный в данной статье алгоритм декомпозиции расчетной сетки обеспечивал равномерное распределение ячеек по доменам, само время обработки ячейки сильно зависит от ее физических свойств и может отличаться в разы.
По этой причине сбалансировать вычислительную нагрузку на разные вычислительные узлы возможно только в динамическом режиме, что не делалось в рамках данного исследования.
Также отметим высокие показатели эффективности масштабирования вычислений для вычислительных узлов на базе микропроцессоров Xeon Cascade Lake.
Данные процессоры -- наиболее современные из всего оборудования, участвовавшего в описанном эксперименте.

\section{Conclusion}

Эффективность масштабирования высоконагруженных вычислений является важным аспектом при разработке параллельных приложений и проведении расчетов.
Сегодня, учитывая сложность решаемых научных и инженерных задач и объемы обрабатываемых данных, трудно расчитывать на эффективность работы на локальной станции или сервере.
Для проведения качественных исследований с использованием современных математических моделей требуется использование суперкомпьютеров.
Для их эффективного использования необходимо уметь создавать приложения, способные выполняться параллельно на многих вычислительных узлах.
Для оценки эффективности запуска параллельных приложений удобно использовать показатель, называемый эффективность масштабирования вычислений, близость которого к единице сигнализирует о том, что используемые подходы и методы организации высокопроизводительных вычислений здраво отражают потребности задачи и аппаратных средств.
В статье описаны различные аспекты, которые имеют решающее значение для достижения высоких показателей эффективности масштабирования вычислений.
Среди них декомпозиция расчетной сетки, механизм организации межпроцессных обменов в процессе счета и выстраивание всей цепочки вычислений в единую последовательность действий.
Для получения практических результатов в ходе исследования была использована задача расчета физических процессов на поверхности обтекаемого тела, вся работа выполнялась на неструктурированной поверхностной расчетной сетке.
Для выполнения запусков использовались несколько сегментов вычислительной системы МСЦ РАН, наиболее высокий показатель эффективности масштабирования вычислений среди которых был достигнут на сегменте на базе микропроцессоров Xeon Cascade Lake.

\begin{acknowledgments}
The work has been done at the JSCC RAS as part of the state assignment for the topic 0580-2021-0016.
The supercomputer MVS-10P OP (Broadwell, KNL and Cascade Lake segments), located at the JSCC RAS, was used during the research.
\end{acknowledgments}

\begin{thebibliography}{99}

\bibitem{Rettinger}
\refitem{article}
C. Rettinger, C. Godenschwager, S. Eibl, et al., {\it ``Fully Resolved Simulations of Dune Formation in Riverbeds"}, ISC High Performance , LNCS~{\bf 10266}, 3--21 (2017).

\bibitem{Krappel}
\refitem{article}
T. Krappel, S. Riedelbauch, {\it ``Scale Resolving Flow Simulations of a Francis Turbine Using Highly Parallel CFD Simulations"}, High Performance Computing in Science and Engineering'16, 499--510 (2016).

\bibitem{Markidis}
\refitem{article}
S. Markidis, I. B. Peng, J. L. Tr\"aff, et al.,{\it ``The EPiGRAM Project: Preparing Parallel Programming Models for Exascale"}, ISC High Performance Workshops, LNCS~{\bf 9945}, 56--68  (2016).

\bibitem{Klenk}
\refitem{article}
B.~Klenk, H.~Fr\"oning, {\it ``An Overview of MPI Characteristics of Exascale Proxy Applications"}, ISC High Performance, LNCS~{\bf 10266}, 217--236  (2016).

\bibitem{Abduljabbar}
\refitem{article}
M.~Abduljabbar, G.~S.~Markomanolis, H.~Ibeid, et al., {\it ``An Overview of MPI Characteristics of Exascale Proxy Applications"}, ISC High Performance, LNCS~{\bf 10266}, 79--96 (2017).

\bibitem{Rybakov}
\refitem{article}
A.~A.~Rybakov, {\it ``Inner respresentation and crossprocess exchange mechanism for block-structured grid for supercomputer calculations"}, Program systems: Theory and Application~{\bf 32}(8:1), 121--134 (2017).

\bibitem{Van}
\refitem{article}
R.~F.~Van der Wijngaart, E.~Georganas,~T.~G.~Mattson, et al., {\it ``A New Parallel Research Kernel to Expand Research on Dynamic Load-Balancing Capabilities"}, ISC High Performance, LNCS~{\bf 10266}, 256--274 (2017).

\bibitem{Benderskiy}
\refitem{article}
L.~A.~Benderskiy, D.~A.~Lyubimov, A.~A.~Rybakov, {\it ``Analysis of scaling efficiency in high-speed turbulent flow calculations on a RANS / ILES supercomputer using the high resolution method"}, Trudy SRISA RAS~{\bf 7}(4), 32--40 (2017).

\bibitem{Heller}
\refitem{article}
T.~Heller, H.~Kaiser, P.~Diehl et al., {\it ``Closing the Performance Gap with Modern C++"}, ISC High Performance, LNCS~{\bf 9945}, 18--31 (2016).

\bibitem{Roganov}
\refitem{article}
Roganov V., Osipov V., Matveev G., {\it ``Solving the 2D Poisson PDE by Gauss-Seidel method with parallel programming system"}, Program systems: theory and applications~{\bf 30}(7:3), 99--107 (2016).

% References for REVIEW OF RESEARCH PAPERS section.

\bibitem{Jeffers_KNL}
\refitem{book}
J.~Jeffers, J.~Reinders, A.~Sodani, \emph{Intel Xeon Phi Processor High Performance Programming, Knights Landing Edition} (Morgan Kaufmann, 2016).

\bibitem{Jeffers_KNC}
\refitem{book}
J.~Jeffers, J.~Reinders, \emph{Intel Xeon Phi Coprocessor Processor High Performance Programming} (Morgan Kaufmann, 2013).

\bibitem{Dorris}
\refitem{article}
J.~Dorris, J.~Kurzak , P.~Luszczek, {\it ``Task-Based Cholesky Decomposition on Knights Corner Using OpenMP"}, ISC High Performance, LNCS~{\bf 9945}, 544--562 (2016).

\bibitem{Tobin}
\refitem{article}
J.~Tobin, A.~Breuer, A.~Heinecke et al., {\it ``Accelerating Seismic Simulations Using the Intel Xeon Phi Knights Landing Processor"}, ISC High Performance, LNCS~{\bf 10266}, 139--157 (2017).

\bibitem{McDoniel}
\refitem{article}
W.~McDoniel, M.~Hohnerbach, R.~Canales et al., {\it ``LAMMPS' PPPM Long-Range Solver for the Second Generation Xeon Phi"}, ISC High Performance, LNCS~{\bf 10266}, 61--78 (2017).

\bibitem{Malas}
\refitem{article}
T.~Malas, T.~Kurth, J.~Deslippe, {\it ``Optimization of the Sparse Matrix-Vector Products of an IDR Krylov Iterative Solver in EMGeo for the Intel KNL Manycore Processor"}, ISC High Performance, LNCS~{\bf 9945}, 378--389 (2016).

\bibitem{Krzikalla}
\refitem{article}
O.~Krzikalla, F.~Wende, M.~H\"ohnerbach, {\it ``Dynamic SIMD Vector Lane Scheduling"}, ISC High Performance, LNCS~{\bf 9945}, 354--365 (2016).

\bibitem{Cook}
\refitem{article}
B.~Cook, P.~Maris,M.~Shao, {\it ``High Performance Optimizations for Nuclear Physics Code MFDn on KNL"}, ISC High Performance, LNCS~{\bf 9945}, 366--377 (2016).

\bibitem{Rybakov_Optimization}
\refitem{article}
A.~A.~Rybakov,{\it ``Optimization of the problem of conflict detection with dangerous aircraft movement areas to execute on Intel Xeon Phi"}, Programmnye produkty i sistemy [Software \& Systems]~{\bf 30}(3), 524--528 (2017).

\bibitem{Sengupta}
\refitem{article}
D.~Sengupta,~Y.~Wang,~N.~Sundaram et al., {\it ``Performance Incremental SVM Learning on Intel Xeon Phi Processors"}, ISC High Performance, LNCS~{\bf 10266}, 120--138 (2017).

\bibitem{Kronbichler}
\refitem{article}
M.~Kronbichler,~K.~Kormann ,~I.~Pasichnyk, {\it ``Fast Matrix-Free Discontinuous Galerkin Kernels on Modern Computer Architectures"}, ISC High Performance, LNCS~{\bf 10266}, 237--255 (2017).

\bibitem{Doerfler}
\refitem{article}
D.~Doerfler,~J.~Deslippe ,~S.~Williams, {\it `Applying the Roofline Performance Model to the Intel Xeon Phi Knights Landing Processor"}, ISC High Performance, LNCS~{\bf 9945}, 339--353 (2016).

\bibitem{Rosales}
\refitem{article}
C.~Rosales, J.~Cazes, K.~Milfeld, {\it ``Comparative Study of Application Performance and Scalability on the Intel Knights Landing Processor"}, ISC High Performance, LNCS~{\bf 9945}, 307--318 (2016).

% References for FLAT CYCLES section.

\bibitem{Intel_SDM}
\refitem{manual}
Intel 64 and IA-32 Architectures Software Developer's Manual, Combined Volumes: 1, 2A, 2B, 2C, 2D, 3A, 3B, 3C, 3D and 4, Intel Corporation (2017).

\bibitem{Intel_C}
\refitem{manual}
Intel C++ Compiler 16.0 User and Reference Guide, Intel Corporation (2016).

\bibitem{Intel_Intr}
\refitem{misc}
Intel Intrinsics Guide. \url{https://software.intel.com/sites/landingpage/IntrinsicsGuide/}. Accessed 2018.

\bibitem{Scott_Predct}
\refitem{article}
S.~A.~Mahlke, D.~C.~Lin, W.~Y.~Chen, R.~E.~Hank, {\it ``Effective Compiler Support for Predicated Execution Using the Hyperblock"}, Proceedings of the 25th International Symposium on Microarchitecture, ~45--54 (1992).

\bibitem{Hwu_Predct}
\refitem{article}
W.~W.~Hwu, {\it ``The Superblock: an Effective Technique for VLIW and Superscalar Compilation"}, The Journal of Supercomputing~{\bf 7}(1/2), ~229--248 (1993).

\bibitem{Golub}
\refitem{book}
G.~H.~Golub, C.~F.~Van Loan, {\it ``Matrix Computations"}, (The John Hopkins University Press, 1989).

\bibitem{Zhang}
\refitem{article}
H.~Zhang, R.~T.~Mills, K.~Rupp, B.~F.~Smith, {\it ``Vectorized Parallel Sparse Matrix-Vector Multiplication in PETSc Using AVX-512"}, Proceedings of the 47th International Conference on Parallel Processing (ICPP 2018), ACM, Article 55, 10 pages (2018).

\bibitem{Lyub_RANS_ILES}
\refitem{article}
D.~A.~Lyubimov, {\it ``Development and Application of a High-Resolution Technique for Jet Flow Computation Using Large Eddy Simulation"}, High Temperature~{\bf 50}(3),~420--436 (2012).

\bibitem{Ben_Lyub_Chest_RANS_ILES}
\refitem{article}
L.~A.~Benderskii, D.~A.~Lyubimov, A.~O.~Chestnykh, B.~M.~Shabanov and A.~A.~Rybakov, {\it ``The Use of the RANS/ILES Method to Study the Influence of Coflow Wind on the Flow in a Hot, Nonisobaric, Supersonic Airdrome Jet during Its Interaction with the Jet Blast Deflector"}, High Temperature~{\bf 56}(2),~247--254 (2018).

\bibitem{Aleen}
\refitem{article}
F.~Aleen, V.~P.~Zakharin, R.~Krishnaiyer, G.~Gupta, D.~Kreitzer, C.-S.~Lin, {\it ``Automated Compiler Optimization of Multiple Vector Loads/Stores"}, International Journal of Parallel Programming~{\bf 46}(2),~471--503 (2018).

% References for IRREGULAR ITERATIONS LOOPS section.

\bibitem{Fast_Sort}
\refitem{article}
B.~Bramas, {\it ``Fast Sorting Algorithms Using AVX-512 on Intel Knights Landing"}, arXiv: 1704.08579, Accessed 2018.

\bibitem{Quick_Sort}
\refitem{article}
S.~Gueron, V.~Krasnov, {\it ``Fast Quicksort Implementation Using AVX Instructions"}, The Computer Journal,~{\bf 59}(1),~83--90 (2016).

\bibitem{Quick_Sort_2}
\refitem{article}
B.~Bramas, {\it ``A Novel Hybrid Quicksort Algorithm Vectorized Using AVX-512 on Intel Skylake"}, International Journal of Advanced Computer Science and Applications (IJACSA)~{\bf 8}(10), (2017).

\bibitem{Knuth}
\refitem{book}
D.~E.~Knuth, {\it ``The Art of Computer Programming: Volume 3: Sorting and Searching (2nd Edition)"}, (Addison-Wesley Professional, 1998).

% References for PHYSICAL CALCULATIONS section.

\bibitem{Toro}
\refitem{book}
E.~F.~Toro, {\it ``Riemann Solvers and Numerical Methods for Fluid Dynamics:
A Practical Introduction, 2nd Edition"}, (Springer,1999).

\bibitem{Numerica}
\refitem{misc}
NUMERICA, A Library of Sources for Teaching, Research and Applications, by E.~F.~Toro. \url{https://github.com/dasikasunder/NUMERICA}. Accessed 2018.

\end{thebibliography}

\end{document}
